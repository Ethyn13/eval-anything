default:
  # Evaluation configurations
  eval_cfgs:
    # Output directory name
    output_dir: null
    # Unique identifier for current task
    task_uid: null
    # Num shot
    n_shot: 0
    # Chain of thought
    cot: False
    # Evaluation metrics
    metrics: {
      "MMMU": ['accuracy', 'f1', 'precision', 'recall', 'roc_auc'],
      "MME": []
      }
  data_cfgs:
    # benchmark name and task name, if the task list is empty, all tasks will be evaluated
    eval_data: {
      "MMMU": ['Accounting', 'Agriculture', 'Architecture_and_Engineering', 'Art', 'Art_Theory', 'Basic_Medical_Science', 'Biology', 'Chemistry', 'Clinical_Medicine', 'Computer_Science', 'Design', 'Diagnostics_and_Laboratory_Medicine', 'Economics', 'Electronics', 'Energy_and_Power', 'Finance', 'Geography', 'History', 'Literature', 'Manage', 'Marketing', 'Materials', 'Math', 'Mechanical_Engineering', 'Music', 'Pharmacy', 'Physics', 'Psychology','Public_Health', 'Sociology'], 
      "MME": []
      }
  # The model configurations
  model_cfgs:
    # Pretrained model name or path
    model_name_or_path: null
    # Model type ("LM" or "MM")
    model_type: "MM"
    # Chat template
    chat_template: null
  infer_cfgs:
    # Inference backend
    infer_backend: "vllm"
    # Whether to trust remote code
    trust_remote_code: True
    # The max token length
    model_max_length: 2048
    # Top-K
    top_k: 50
    # Top-P
    top_p: 0.95
    # Temperature
    temperature: 0.5
    # Beam Search
    beam_search: False
    # Beam Search size
    num_beams: null
    # Number of GPUs
    num_gpu: 8
    # Available GPU IDs. If not specified, first num_gpu GPUs will be used.
    gpu_ids: []
    # GPU utilization
    gpu_utilization: 0.8
    