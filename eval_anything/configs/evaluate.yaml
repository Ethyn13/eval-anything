  # Evaluation configurations
  eval_cfgs:
    # Output directory
    output_dir: ../output
    # Cache directory, if not specified, cache will not be saved
    cache_dir: ../cache
    # Unique identifier for current task
    task_uid: 111
    # benchmark name and task name, if the task list is empty, all tasks will be evaluated
    benchmarks: {
      #"gsm8k": ["main"],
      #"HumanEval": ["openai_humaneval"],
      "mmlu": ["abstract_algebra", "anatomy", "astronomy", "business_ethics", "clinical_knowledge", "college_biology", "college_chemistry", "college_computer_science", "college_mathematics", "college_medicine", "college_physics", "computer_security", "conceptual_physics", "econometrics", "electrical_engineering", "elementary_mathematics", "formal_logic", "global_facts", "high_school_biology", "high_school_chemistry", "high_school_computer_science", "high_school_european_history", "high_school_geography", "high_school_government_and_politics", "high_school_macroeconomics", "high_school_mathematics", "high_school_microeconomics", "high_school_physics", "high_school_psychology", "high_school_statistics", "high_school_us_history", "high_school_world_history", "human_aging", "human_sexuality", "international_law", "jurisprudence", "logical_fallacies", "machine_learning", "management", "marketing", "medical_genetics", "miscellaneous", "moral_disputes", "moral_scenarios", "nutrition", "philosophy", "prehistory", "professional_accounting", "professional_law", "professional_medicine", "professional_psychology", "public_relations", "security_studies", "sociology", "us_foreign_policy", "virology", "world_religions"],
      }
    # Num shot
    n_shot: {
      #"gsm8k": 5,
      #"HumanEval": 0,
      "mmlu": 0,
      }
    # Chain of thought
    cot: {
      #"gsm8k": True,
      #"HumanEval": False,
      "mmlu": False,
      }
  # The model configurations
  model_cfgs:
    # Pretrained model unique identity
    model_id: Llama3.1-8B-Instruct
    # Pretrained model name or path
    model_name_or_path: /aifs4su/yaodong/models/llama3.1/Meta-Llama-3.1-8B-Instruct
    # Model type ("LM" or "MM")
    model_type: "LM"
    # Chat template
    chat_template: Llama3
  infer_cfgs:
    # Inference backend
    infer_backend: "vllm"
    # Whether to trust remote code
    trust_remote_code: True
    # The max token length
    model_max_length: 2048
    # The max new tokens
    max_new_tokens: 512
    # The number of Output
    num_output: 1
    # Top-K
    top_k: 50
    # Top-P
    top_p: 0.95
    # Temperature
    temperature: 0.
    # Prompt Logprobs
    prompt_logprobs: 0
    # Logprobs
    logprobs: 20
    # Beam Search
    beam_search: False
    # Beam Search size
    num_beams: null
    # Number of GPUs
    num_gpu: 8
    # Available GPU IDs. If not specified, first num_gpu GPUs will be used.
    gpu_ids: [0,1,2,3,4,5,6,7]
    # GPU utilization
    gpu_utilization: 0.8
    